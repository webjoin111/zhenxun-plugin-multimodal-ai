from typing import TYPE_CHECKING

if TYPE_CHECKING:
    pass
from nonebot.adapters.onebot.v11 import (
    Bot,
    GroupMessageEvent,
    MessageEvent,
    MessageSegment,
)
from nonebot_plugin_alconna import CommandResult, UniMessage, UniMsg
from nonebot_plugin_alconna.uniseg import Image as UniImage
from nonebot_plugin_alconna.uniseg import Video as UniVideo
from nonebot_plugin_alconna.uniseg import Voice as UniVoice

from zhenxun.services.llm import (
    LLMMessage,
    get_model_instance,
    message_to_unimessage,
    unimsg_to_llm_parts,
)
from zhenxun.services.llm.types import get_user_friendly_error_message
from zhenxun.services.log import logger

from .. import ai
from ..config import CHINESE_CHAR_THRESHOLD, base_config
from ..core import get_current_active_model_name
from ..core.agent_loop import run_generic_agent_loop
from ..core.session_manager import (
    SessionStatus,
    session_manager,
)
from ..tools import (
    MCP_AVAILABLE,
    detect_function_calling_intent_with_ai,
    detect_intent_by_keywords,
)
from ..utils.converters import convert_to_image

MARKDOWN_STYLING_PROMPT = """
æ³¨æ„ä½¿ç”¨ä¸°å¯Œçš„markdownæ ¼å¼è®©å†…å®¹æ›´ç¾è§‚ï¼Œæ³¨æ„è¦åœ¨åˆé€‚çš„åœºæ™¯ä½¿ç”¨åˆé€‚çš„æ ·å¼,ä¸åˆé€‚å°±ä¸ä½¿ç”¨,åŒ…æ‹¬ï¼š
æ ‡é¢˜å±‚çº§(h1-h6)ã€ç²—ä½“(bold)ã€æ–œä½“(em)ã€å¼•ç”¨å—(blockquote)ã€
æœ‰åºåˆ—è¡¨(ordered list)ã€æ— åºåˆ—è¡¨(unordered list)ã€ä»»åŠ¡åˆ—è¡¨(checkbox)ã€
ä»£ç å—(code)ã€å†…è”ä»£ç (inline code)ã€è¡¨æ ¼(table)ã€åˆ†éš”çº¿(hr)ã€
åˆ é™¤çº¿(Strikethrough)ã€é“¾æ¥(links)ã€åµŒå¥—åˆ—è¡¨(nested lists)ã€emojiå¢å¼ºæ ¼å¼(emoji-enhanced formatting)ã€
Mermaidå›¾è¡¨(graph td)ç­‰ã€‚
""".strip()


AGENT_CONFIGS = {
    "MAP": {
        "tool_name": "baidu-map",
        "system_prompt": """ä½ æ˜¯ä¸“ä¸šçš„åœ°ç†å’Œè·¯çº¿è§„åˆ’åŠ©æ‰‹ï¼Œæ‹¥æœ‰ç™¾åº¦åœ°å›¾åŠŸèƒ½ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
â€¢ åœ°å€åæ ‡è½¬æ¢ï¼ˆåœ°å€â†”åæ ‡ï¼‰
â€¢ åœ°ç‚¹æŸ¥è¯¢ï¼ˆPOIæ£€ç´¢ã€è¯¦æƒ…è·å–ï¼‰
â€¢ è·¯çº¿è§„åˆ’ï¼ˆå¤šç§å‡ºè¡Œæ–¹å¼ã€è·ç¦»æ—¶é—´ï¼‰
â€¢ å®æ—¶ä¿¡æ¯ï¼ˆè·¯å†µã€å¤©æ°”ã€å®šä½ï¼‰

é‡è¦æé†’ï¼š
- å¿…é¡»ä½¿ç”¨å·¥å…·è·å–å®æ—¶å‡†ç¡®ä¿¡æ¯ï¼Œä¸è¦å‡­è®°å¿†å›ç­”
- å·¥å…·å¤±è´¥æ—¶è¯´æ˜åŸå› å¹¶æä¾›å»ºè®®
- ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼æ¸…æ™°ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯

{MARKDOWN_STYLING_PROMPT}""",
    },
}


async def _prepare_final_response(response: str | bytes) -> str | MessageSegment:
    """ç»Ÿä¸€å¤„ç†æœ€ç»ˆçš„å“åº”ï¼Œå‡†å¤‡å¾…å‘é€çš„æ¶ˆæ¯æ®µï¼ŒåŒ…æ‹¬Markdownè½¬å›¾ç‰‡é€»è¾‘"""
    if isinstance(response, bytes):
        return MessageSegment.image(response)

    if base_config.get("enable_md_to_pic"):
        if (
            sum(1 for c in response if "\u4e00" <= c <= "\u9fff")
            >= CHINESE_CHAR_THRESHOLD
        ):
            try:
                image_data = await convert_to_image(response)
                if image_data:
                    return MessageSegment.image(image_data)
            except Exception as e:
                if e.__class__.__name__ != "FinishedException":
                    logger.error(f"Markdownè½¬å›¾ç‰‡å¤±è´¥: {e}")
                    return f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œå·²å›é€€åˆ°æ–‡æœ¬æ˜¾ç¤º:\n{response}"

    return response


@ai.handle()
async def chat_handler(
    bot: Bot, event: MessageEvent, result: CommandResult, msg: UniMsg
):
    user_id_str = event.get_user_id()

    try:
        main_args = result.result.main_args if result.result.main_args else {}
        query_segments = main_args.get("query", [])
        query = UniMessage(query_segments).extract_plain_text().strip()
        logger.debug(f"æå–çš„æŒ‡ä»¤æ–‡æœ¬: '{query}'")

        full_message = UniMessage(query_segments)

        if event.reply and event.reply.message:
            logger.debug("æ£€æµ‹åˆ°å¼•ç”¨æ¶ˆæ¯ï¼Œæ­£åœ¨ä½¿ç”¨ä¾¿æ·æ–¹æ³•è½¬æ¢å¹¶åˆå¹¶å†…å®¹...")
            reply_unimessage = message_to_unimessage(event.reply.message)
            full_message = reply_unimessage + full_message
            logger.debug(f"åˆå¹¶åçš„å®Œæ•´æ¶ˆæ¯åŒ…å« {len(full_message)} ä¸ªæ®µã€‚")

            if not query:
                updated_query = full_message.extract_plain_text().strip()
                if updated_query:
                    query = updated_query
                    logger.debug(f"ä»åˆå¹¶æ¶ˆæ¯ä¸­æ›´æ–°æŸ¥è¯¢æ–‡æœ¬: '{query}'")
        else:
            logger.debug("æœªæ£€æµ‹åˆ°å¼•ç”¨æ¶ˆæ¯ã€‚")

        media_types_to_check = (UniImage, UniVideo, UniVoice)
        has_media = any(full_message.has(t) for t in media_types_to_check)
        if not query and not has_media:
            await ai.finish("è¯·æä¾›é—®é¢˜æˆ–é™„å¸¦å›¾ç‰‡ã€æ–‡ä»¶ç­‰å†…å®¹ã€‚")
            return

        if has_media:
            active_model_name = get_current_active_model_name()
            if not active_model_name:
                await ai.finish("é”™è¯¯ï¼šå½“å‰æœªé…ç½®ä»»ä½•AIæ¨¡å‹ã€‚")
                return

            async with await get_model_instance(active_model_name) as model_instance:
                unsupported_media = []
                if (
                    full_message.has(UniImage)
                    and not model_instance.can_process_images()
                ):
                    unsupported_media.append("å›¾ç‰‡")
                if (
                    full_message.has(UniVideo)
                    and not model_instance.can_process_video()
                ):
                    unsupported_media.append("è§†é¢‘")
                if (
                    full_message.has(UniVoice)
                    and not model_instance.can_process_audio()
                ):
                    unsupported_media.append("éŸ³é¢‘")

                if unsupported_media:
                    media_types_str = "ã€".join(unsupported_media)
                    error_message = (
                        f"å½“å‰æ¨¡å‹ `{active_model_name}` ä¸æ”¯æŒå¤„ç† {media_types_str} å†…å®¹ã€‚"
                        f"è¯·åˆ‡æ¢åˆ°æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹æˆ–å‘é€çº¯æ–‡æœ¬ã€‚"
                    )
                    logger.warning(
                        f"ç”¨æˆ· {user_id_str} å°è¯•ä½¿ç”¨ä¸æ”¯æŒçš„åª’ä½“ç±»å‹ ({media_types_str}) "
                        f"ä¸æ¨¡å‹ {active_model_name}."
                    )
                    await ai.finish(error_message)
                    return

        group_id = str(event.group_id) if isinstance(event, GroupMessageEvent) else None

        session_state = session_manager.get_or_create_session(user_id_str, group_id)

        if session_state.status == SessionStatus.PROCESSING_AGENT:
            await ai.finish("æˆ‘æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»å†å‘é€æ¶ˆæ¯å“¦~")
            return

        response: str | bytes
        ai_instance = session_state.ai_instance
        current_intent = session_state.intent

        if (
            session_state.status == SessionStatus.AWAITING_USER_INPUT
            and current_intent in AGENT_CONFIGS
        ):
            logger.info(
                f"ğŸš¦ æ£€æµ‹åˆ°ç”¨æˆ·æ­£å¤„äº {current_intent} ä»»åŠ¡ä¸­ï¼Œç»§ç»­Agentå¾ªç¯..."
            )
            ai_instance.history.append(
                LLMMessage.user(await unimsg_to_llm_parts(full_message))
            )
            agent_config = AGENT_CONFIGS[current_intent]
            response = await run_generic_agent_loop(
                session_state=session_state,
                mcp_tool_name=agent_config["tool_name"],
                system_prompt=agent_config["system_prompt"],
                bot=bot,
                event=event,
                model_name=base_config.get("AGENT_MODEL_NAME"),
            )
        else:
            if session_state.status != SessionStatus.IDLE:
                logger.warning(
                    f"ä¼šè¯çŠ¶æ€ä¸º {session_state.status.value} ä½†æœªè¢«å¤„ç†ï¼Œé‡ç½®ä¸ºIDLEã€‚"
                )
                session_state.status = SessionStatus.IDLE
                session_state.intent = None

            if base_config.get("enable_ai_intent_detection"):
                intent_result = await detect_function_calling_intent_with_ai(query)
            else:
                intent_result = detect_intent_by_keywords(query)

            intent = intent_result.get("intent")
            logger.info(
                f"ğŸ§  æ„å›¾æ£€æµ‹: {intent} (ç½®ä¿¡åº¦: {intent_result.get('confidence', 1.0):.2f}) "
                f"| AIæ£€æµ‹: {'å·²å¯ç”¨' if base_config.get('enable_ai_intent_detection') else 'å·²ç¦ç”¨'}"
            )

            if (
                intent in AGENT_CONFIGS
                and base_config.get("enable_mcp_tools")
                and MCP_AVAILABLE
            ):
                logger.info(f"ğŸš¦ è·¯ç”±åˆ°é€šç”¨ Agent å¾ªç¯å¤„ç†: {intent}")
                ai_instance.history.append(
                    LLMMessage.user(await unimsg_to_llm_parts(full_message))
                )
                session_state.intent = intent
                agent_config = AGENT_CONFIGS[intent]
                response = await run_generic_agent_loop(
                    session_state=session_state,
                    mcp_tool_name=agent_config["tool_name"],
                    system_prompt=agent_config["system_prompt"],
                    bot=bot,
                    event=event,
                    model_name=base_config.get("AGENT_MODEL_NAME"),
                )
            elif intent == "SEARCH":
                logger.info("ğŸš¦ è·¯ç”±åˆ°å†…ç½®æœç´¢: SEARCH")
                search_instruction = (
                    f"è¯·ç”¨ä¸­æ–‡æœç´¢å¹¶è¯¦ç»†å›ç­”ã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š'{query}'\n"
                    f"{MARKDOWN_STYLING_PROMPT}"
                )
                search_result = await ai_instance.search(
                    full_message, instruction=search_instruction
                )

                if search_result.get("success", False):
                    response_text = search_result.get("text", "")
                    sources = search_result.get("sources", [])
                    queries = search_result.get("queries", [])

                    if queries:
                        response_text += "\n\nğŸ” æœç´¢æŸ¥è¯¢ï¼š"
                        for i, query_text in enumerate(queries[:3], 1):
                            response_text += f"\n{i}. {query_text}"

                    if sources:
                        response_text += "\n\nğŸ“š ä¿¡æ¯æ¥æºï¼š"
                        for i, source in enumerate(sources[:5], 1):
                            title = getattr(source, "title", "æœªçŸ¥æ¥æº")
                            uri = getattr(source, "uri", "")
                            response_text += f"\n{i}. {title}" + (
                                f" - {uri}" if uri else ""
                            )

                    logger.info(
                        f"âœ… æœç´¢æˆåŠŸï¼Œæ¥æºæ•°é‡: {len(sources)}, æŸ¥è¯¢æ•°é‡: {len(queries)}"
                    )
                    response = f"ğŸ” æœç´¢ç»“æœï¼š\n{response_text}"
                else:
                    logger.warning("æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šåˆ†ææ¨¡å¼")
                    fallback_prompt = (
                        f"è¯·ç”¨ä¸­æ–‡è¯¦ç»†å›ç­”å…³äº '{query}' çš„é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ä¿¡æ¯ã€‚\n"
                        f"{MARKDOWN_STYLING_PROMPT}"
                    )
                    llm_response = await ai_instance.analyze(
                        full_message, instruction=fallback_prompt
                    )
                    response = llm_response.text
            else:
                logger.info("ğŸš¦ è·¯ç”±åˆ°ç®€å•å¯¹è¯: CHAT")

                content_parts = await unimsg_to_llm_parts(full_message)

                if not ai_instance.history:
                    chat_instruction = f"è¯·ç”¨ä¸­æ–‡å›å¤ã€‚\n{MARKDOWN_STYLING_PROMPT}"
                    ai_instance.history.append(LLMMessage.system(chat_instruction))

                llm_response_obj = await ai_instance.chat(content_parts or "")
                response = llm_response_obj.text

        final_message_to_send = await _prepare_final_response(response)
        await ai.finish(final_message_to_send)

    except Exception as e:
        if e.__class__.__name__ != "FinishedException":
            logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
            friendly_message = get_user_friendly_error_message(e)
            await ai.finish(f"å¤„ç†è¯·æ±‚å¤±è´¥: {friendly_message}")
