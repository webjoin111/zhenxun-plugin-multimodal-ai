from typing import TYPE_CHECKING

from nonebot.matcher import Matcher
from nonebot.params import Depends

if TYPE_CHECKING:
    pass
from nonebot.adapters.onebot.v11 import (
    Bot,
    GroupMessageEvent,
    MessageEvent,
    MessageSegment,
)
from nonebot_plugin_alconna import CommandResult, UniMessage, UniMsg
from nonebot_plugin_alconna.uniseg import Image as UniImage
from nonebot_plugin_alconna.uniseg import Video as UniVideo
from nonebot_plugin_alconna.uniseg import Voice as UniVoice

from zhenxun.services.llm import (
    get_model_instance,
    message_to_unimessage,
    unimsg_to_llm_parts,
    LLMException,
    LLMGenerationConfig,
    LLMMessage,
)
from zhenxun.services.llm.types import get_user_friendly_error_message
from zhenxun.services.log import logger

from zhenxun import ui
from .. import ai
from ..config import (
    CHINESE_CHAR_THRESHOLD,
    CSS_DIR,
    MARKDOWN_STYLING_PROMPT,
    base_config,
)
from ..core import get_current_active_model_name

from ..core.session_manager import session_manager

from ..core.intent import (
    detect_function_calling_intent_with_ai,
    detect_intent_by_keywords,
)


async def _handle_search_intent(ai_instance, message: UniMessage, query: str) -> str:
    """å¤„ç† SEARCH æ„å›¾ï¼šç›´æ¥ã€å•æ¬¡è°ƒç”¨ Gemini Groundingã€‚"""
    logger.info("ğŸš¦ è·¯ç”±ç­–ç•¥: SEARCH (ç›´æ¥è°ƒç”¨)")
    search_instruction = (
        f"è¯·ç”¨ä¸­æ–‡æœç´¢å¹¶è¯¦ç»†å›ç­”ã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š'{query}'\n{MARKDOWN_STYLING_PROMPT}"
    )

    try:
        search_response = await ai_instance.search(
            message, instruction=search_instruction
        )

        response_text = search_response.text

        if search_response.grounding_metadata:
            sources = search_response.grounding_metadata.grounding_attributions or []
            queries = search_response.grounding_metadata.web_search_queries or []

            if sources:
                response_text += "\n\nğŸ“š **å‚è€ƒæ¥æºï¼š**\n"
                for i, source in enumerate(sources[:3], 1):
                    title = source.title or "æœªçŸ¥æ ‡é¢˜"
                    url = source.uri or ""
                    if url:
                        response_text += f"{i}. [{title}]({url})\n"
                    else:
                        response_text += f"{i}. {title}\n"

            if queries:
                response_text += f"\nğŸ” **æœç´¢æŸ¥è¯¢ï¼š** {', '.join(queries)}"

        return f"ğŸ” æœç´¢ç»“æœï¼š\n{response_text}"

    except LLMException as e:
        logger.warning(f"æœç´¢å¤±è´¥: {e.user_friendly_message}", e=e)
        return f"æŠ±æ­‰ï¼Œæœç´¢åŠŸèƒ½å½“å‰ä¼¼ä¹å‡ºäº†ç‚¹é—®é¢˜ï¼š{e.user_friendly_message}"
    except Exception as e:
        logger.error("å¤„ç†æœç´¢æ„å›¾æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯", e=e)
        return "æŠ±æ­‰ï¼Œæœç´¢åŠŸèƒ½å½“å‰ä¼¼ä¹å‡ºäº†ç‚¹é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"


async def _prepare_final_response(response: str | bytes) -> str | MessageSegment:
    """ç»Ÿä¸€å¤„ç†æœ€ç»ˆçš„å“åº”ï¼Œå‡†å¤‡å¾…å‘é€çš„æ¶ˆæ¯æ®µï¼ŒåŒ…æ‹¬Markdownè½¬å›¾ç‰‡é€»è¾‘"""
    if isinstance(response, bytes):
        return MessageSegment.image(response)
    assert isinstance(response, str)
    if base_config.get("enable_md_to_pic"):
        if (
            sum(1 for c in response if "\u4e00" <= c <= "\u9fff")  # type: ignore
            >= CHINESE_CHAR_THRESHOLD
        ):
            try:
                theme_name = base_config.get("THEME", "light")
                css_path = CSS_DIR / f"{theme_name}.css"
                md_component = ui.markdown(response)
                md_component.css_path = str(css_path.absolute())
                md_component.component_css = """
                body {
                    padding: 20px;
                    line-height: 1.6;
                }
                """
                image_data = await ui.render(md_component)
                if image_data:
                    return MessageSegment.image(image_data)
            except Exception as e:
                if e.__class__.__name__ != "FinishedException":
                    logger.error(f"Markdownè½¬å›¾ç‰‡å¤±è´¥: {e}")
                    return f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œå·²å›é€€åˆ°æ–‡æœ¬æ˜¾ç¤º:\n{response}"

    return response


@ai.handle()
async def chat_handler(
    bot: Bot,
    event: MessageEvent,
    result: CommandResult,
    msg: UniMsg,
    matcher: Matcher = Depends(),
):
    user_id_str = event.get_user_id()

    try:
        main_args = result.result.main_args if result.result.main_args else {}
        query_segments = main_args.get("query", [])
        query = UniMessage(query_segments).extract_plain_text().strip()
        logger.debug(f"æå–çš„æŒ‡ä»¤æ–‡æœ¬: '{query}'")

        full_message = UniMessage(query_segments)

        if event.reply and event.reply.message:
            logger.debug("æ£€æµ‹åˆ°å¼•ç”¨æ¶ˆæ¯ï¼Œæ­£åœ¨ä½¿ç”¨ä¾¿æ·æ–¹æ³•è½¬æ¢å¹¶åˆå¹¶å†…å®¹...")
            reply_unimessage = message_to_unimessage(event.reply.message)
            full_message = reply_unimessage + full_message
            logger.debug(f"åˆå¹¶åçš„å®Œæ•´æ¶ˆæ¯åŒ…å« {len(full_message)} ä¸ªæ®µã€‚")

            if not query:
                updated_query = full_message.extract_plain_text().strip()
                if updated_query:
                    query = updated_query
                    logger.debug(f"ä»åˆå¹¶æ¶ˆæ¯ä¸­æ›´æ–°æŸ¥è¯¢æ–‡æœ¬: '{query}'")
        else:
            logger.debug("æœªæ£€æµ‹åˆ°å¼•ç”¨æ¶ˆæ¯ã€‚")

        media_types_to_check = (UniImage, UniVideo, UniVoice)
        has_media = any(full_message.has(t) for t in media_types_to_check)
        if not query and not has_media:
            await ai.finish("è¯·æä¾›é—®é¢˜æˆ–é™„å¸¦å›¾ç‰‡ã€æ–‡ä»¶ç­‰å†…å®¹ã€‚")
            return

        if has_media:
            active_model_name = get_current_active_model_name()
            if not active_model_name:
                await ai.finish("é”™è¯¯ï¼šå½“å‰æœªé…ç½®ä»»ä½•AIæ¨¡å‹ã€‚")
                return

            async with await get_model_instance(active_model_name) as model_instance:
                unsupported_media = []
                if (
                    full_message.has(UniImage)
                    and not model_instance.can_process_images()
                ):
                    unsupported_media.append("å›¾ç‰‡")
                if (
                    full_message.has(UniVideo)
                    and not model_instance.can_process_video()
                ):
                    unsupported_media.append("è§†é¢‘")
                if (
                    full_message.has(UniVoice)
                    and not model_instance.can_process_audio()
                ):
                    unsupported_media.append("éŸ³é¢‘")

                if unsupported_media:
                    media_types_str = "ã€".join(unsupported_media)
                    error_message = (
                        f"å½“å‰æ¨¡å‹ `{active_model_name}` ä¸æ”¯æŒå¤„ç† {media_types_str} å†…å®¹ã€‚"
                        f"è¯·åˆ‡æ¢åˆ°æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹æˆ–å‘é€çº¯æ–‡æœ¬ã€‚"
                    )
                    logger.warning(
                        f"ç”¨æˆ· {user_id_str} å°è¯•ä½¿ç”¨ä¸æ”¯æŒçš„åª’ä½“ç±»å‹ ({media_types_str}) "
                        f"ä¸æ¨¡å‹ {active_model_name}."
                    )
                    await ai.finish(error_message)
                    return

        group_id = str(event.group_id) if isinstance(event, GroupMessageEvent) else None

        session_state = session_manager.get_or_create_session(user_id_str, group_id)
        ai_instance = session_state.ai_instance

        if base_config.get("enable_ai_intent_detection"):
            intent_result = await detect_function_calling_intent_with_ai(query)
        else:
            intent_result = detect_intent_by_keywords(query)

        intent = intent_result.get("intent")
        logger.info(f"ğŸ§  æ„å›¾æ£€æµ‹: {intent}")

        generation_config_params = {
            k: v
            for k, v in main_args.items()
            if k in ["temperature", "top_p", "top_k", "max_tokens", "stop_sequences"]
        }

        base_gen_config = (
            LLMGenerationConfig(**generation_config_params)
            if generation_config_params
            else None
        )

        response_text = ""

        user_content_parts = await unimsg_to_llm_parts(full_message)
        current_user_message = LLMMessage.user(user_content_parts or query)

        if intent == "SEARCH":
            response_text = await _handle_search_intent(
                ai_instance, full_message, query
            )
            await ai_instance.add_user_message_to_history(current_user_message)
            await ai_instance.add_assistant_response_to_history(response_text)

        if intent == "CHAT":
            logger.info("ğŸš¦ è·¯ç”±ç­–ç•¥: CHAT (ç›´æ¥è°ƒç”¨ ai.chat)")
            chat_response = await ai_instance.chat(
                current_user_message,
                instruction=MARKDOWN_STYLING_PROMPT,
                **base_gen_config.to_dict() if base_gen_config else {},
            )
            response_text = chat_response.text

        if not response_text:
            response_text = "ä»»åŠ¡å·²æ‰§è¡Œï¼Œä½†AIæ²¡æœ‰æä¾›é¢å¤–çš„æ–‡æœ¬å›å¤ã€‚"

        final_message_to_send = await _prepare_final_response(response_text)
        await ai.finish(final_message_to_send)

    except Exception as e:
        if e.__class__.__name__ != "FinishedException":
            logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
            friendly_message = get_user_friendly_error_message(e)
            await ai.finish(f"å¤„ç†è¯·æ±‚å¤±è´¥: {friendly_message}")
